import os
import torch
from huggingface_hub import snapshot_download
from transformers import CLIPModel

def download_and_convert_clip_model(
    repo_id: str,
    save_dir: str,
    target_filename: str = "ViT-L-14-336px.pt"
):
    """
    ä»Hugging Faceä»“åº“ä¸‹è½½CLIPæ¨¡å‹æƒé‡ï¼Œå¹¶è½¬æ¢ä¸ºæŒ‡å®šçš„.ptæ–‡ä»¶
    
    Args:
        repo_id: Hugging Faceä»“åº“IDï¼ˆopenai/clip-vit-large-patch14-336ï¼‰
        save_dir: æœ€ç»ˆæ–‡ä»¶ä¿å­˜ç›®å½•
        target_filename: åŸä»£ç éœ€è¦çš„ç›®æ ‡æ–‡ä»¶å
    """
    # 1. å®šä¹‰è·¯å¾„
    final_save_path = os.path.join(save_dir, target_filename)
    # ä¸´æ—¶ç¼“å­˜ç›®å½•ï¼ˆä¸‹è½½ä»“åº“æƒé‡ç”¨ï¼‰
    cache_dir = os.path.join(save_dir, "hf_cache")
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(cache_dir, exist_ok=True)

    try:
        # 2. ä»Hugging Faceä»“åº“ä¸‹è½½æ‰€æœ‰æƒé‡æ–‡ä»¶ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        print(f"ğŸ“¥ å¼€å§‹ä» {repo_id} ä¸‹è½½æ¨¡å‹æƒé‡...")
        snapshot_download(
            repo_id=repo_id,
            cache_dir=cache_dir,
            resume_download=True,  # æ–­ç‚¹ç»­ä¼ 
            local_dir_use_symlinks=False  # é¿å…ç¬¦å·é“¾æ¥é—®é¢˜
        )
        print("âœ… ä»“åº“æƒé‡ä¸‹è½½å®Œæˆ")

        # 3. åŠ è½½æ¨¡å‹ï¼ˆä»ç¼“å­˜ç›®å½•åŠ è½½ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼‰
        print("ğŸ”§ åŠ è½½æ¨¡å‹å¹¶è½¬æ¢æ ¼å¼...")
        model = CLIPModel.from_pretrained(
            repo_id,
            cache_dir=cache_dir,
            trust_remote_code=True,  # å…¼å®¹æ–°ç‰ˆtransformers
            local_files_only=True  # ä»…ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„æƒé‡ï¼Œä¸è”ç½‘
        )

        # 4. ä¿å­˜ä¸ºåŸä»£ç éœ€è¦çš„.ptæ–‡ä»¶ï¼ˆä¸¤ç§æ–¹å¼å¯é€‰ï¼Œæ ¹æ®åŸä»£ç é€‚é…ï¼‰
        # æ–¹å¼1ï¼šä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå«ç»“æ„ï¼ŒåŸä»£ç å¯ç›´æ¥torch.loadåŠ è½½ï¼‰
        torch.save(model, final_save_path)
        # æ–¹å¼2ï¼šä»…ä¿å­˜æƒé‡ï¼ˆè‹¥åŸä»£ç æœ‰æ¨¡å‹ç»“æ„å®šä¹‰ï¼Œæ–‡ä»¶æ›´å°ï¼Œæ³¨é‡Šæ‰æ–¹å¼1å¯ç”¨æ­¤æ–¹å¼ï¼‰
        # torch.save(model.state_dict(), final_save_path)

        # 5. éªŒè¯ç»“æœ
        if os.path.exists(final_save_path) and os.path.getsize(final_save_path) > 0:
            print(f"âœ… æ¨¡å‹è½¬æ¢å®Œæˆï¼æœ€ç»ˆæ–‡ä»¶è·¯å¾„ï¼š{final_save_path}")
            print(f"ğŸ“Œ æ–‡ä»¶å¤§å°ï¼š{os.path.getsize(final_save_path)/1024/1024:.1f}MB")
            
            # å¯é€‰ï¼šåˆ é™¤ä¸´æ—¶ç¼“å­˜ï¼ˆèŠ‚çœç©ºé—´ï¼‰
            # import shutil
            # shutil.rmtree(cache_dir)
        else:
            print("âŒ æ¨¡å‹ä¿å­˜å¤±è´¥ï¼šæ–‡ä»¶ä¸ºç©ºæˆ–æœªç”Ÿæˆ")

    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
        print("\nğŸ’¡ æ’æŸ¥å»ºè®®ï¼š")
        print("  1. ç¡®è®¤å·²æ‰§è¡Œ export HF_ENDPOINT=https://hf-mirror.com")
        print("  2. æ£€æŸ¥ç½‘ç»œæ˜¯å¦èƒ½è®¿é—® https://hf-mirror.com")
        print("  3. ç¡®ä¿ç£ç›˜ç©ºé—´è¶³å¤Ÿï¼ˆè‡³å°‘2GBï¼‰")

# ===================== æ‰§è¡Œé…ç½® =====================
if __name__ == "__main__":
    # Hugging Faceä»“åº“IDï¼ˆå›ºå®šä¸ºopenai/clip-vit-large-patch14-336ï¼‰
    REPO_ID = "openai/clip-vit-large-patch14-336"
    # ç›®æ ‡ä¿å­˜ç›®å½•ï¼ˆåŒ¹é…åŸä»£ç æŠ¥é”™è·¯å¾„ï¼‰
    SAVE_DIR = "/root/autodl-tmp/AA-CLIP_add_mvtec2/model/"
    # ç›®æ ‡æ–‡ä»¶åï¼ˆåŸä»£ç éœ€è¦çš„ViT-L-14-336px.ptï¼‰
    TARGET_FILENAME = "ViT-L-14-336px.pt"

    # æ‰§è¡Œä¸‹è½½+è½¬æ¢
    download_and_convert_clip_model(
        repo_id=REPO_ID,
        save_dir=SAVE_DIR,
        target_filename=TARGET_FILENAME
    )